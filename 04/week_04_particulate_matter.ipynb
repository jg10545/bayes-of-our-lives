{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 04:\n",
    "\n",
    "This notebook follows some of the examples in *Visualization in Bayesian Workflow* by Gabry, Simpson, Vehtari, Betancourt, and Gelman.\n",
    "\n",
    "Our goal for this problem is to estimate ground-station values of $PM_{2.5}$ (air concentrations of particulate matter of 2.5 microns or below), with uncertainty intervals, from satellite data. That is, we want to use a high-quality, reliable dataset that suffers from sparse and biased collection to calibrate a noisier dataset that we can collect anywhere. The plan:\n",
    "\n",
    "1. Do some exploratory data analysis\n",
    "* Formulate a model\n",
    "* Evaluate how we've specified the model, before running inference, using samples from the prior predictive distribution\n",
    "* Run inference and inspect the MCMC results visually\n",
    "* Evaluate the model's performance using samples from the posterior predictive distribution\n",
    "* Define a more-complicated model and use these tools to compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pystan\n",
    "from scipy.stats import linregress, skew\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"particulate_data.csv\", encoding=\"iso-8859-1\")\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Since we're relating two variables, expect a lot of scatter plots.\n",
    "\n",
    "Start with the logarithm of the ground-station measurements, plotted against the logarithm of the satellite measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(10,5)\n",
    "slope, intercept, *_ = linregress(data.log_sat, data.log_pm25)\n",
    "full_x = np.array([data.log_sat.min(), data.log_sat.max()])\n",
    "full_y = slope*full_x+intercept\n",
    "\n",
    "plt.plot(data.log_sat, data.log_pm25, \".\", alpha=0.25)\n",
    "plt.plot(full_x, full_y, lw=4)\n",
    "\n",
    "plt.xlabel(\"log(satellite)\", fontsize=14)\n",
    "plt.ylabel(\"log($PM_{2.5}$)\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you squint, the linear regression line seems like an OK(ish) place to start; the data definitely has an overall linear correlation. The focus of the rest of this notebook will be on the extent to which deviations from this linear relationship are going to mess us up.\n",
    "\n",
    "If we look closer, however, the errors don't seem to be totally random (e.g. they're not I.I.D. normal)- the data seems to have some structure to it, with some areas tending to have positive residuals and other areas negative. Let's do some more exploration to see if we can (qualitatively) understand where this is coming from.\n",
    "\n",
    "One possibility is that different *types* of locations have different relationships between satellite and ground station data. We could do a quick check by breaking out the above plot by WHO super-regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,5)\n",
    "plt.plot(full_x, full_y, lw=4)\n",
    "\n",
    "markers = [\".\", \"o\", \"^\", \"s\", \"H\", \"*\", \"v\"]\n",
    "colors = list(matplotlib.colors.TABLEAU_COLORS)#[:7]\n",
    "for i, m, c in zip(data.super_region_name.unique(), markers, colors):\n",
    "    subset = data[data.super_region_name == i]\n",
    "    plt.scatter(subset.log_sat, subset.log_pm25, marker=m, color=c, alpha=0.5, label=i)\n",
    "    \n",
    "    slope, intercept, *_ = linregress(subset.log_sat, subset.log_pm25)\n",
    "    x = np.array([subset.log_sat.min(), subset.log_sat.max()])\n",
    "    plt.plot(x, slope*x+intercept, c, lw=2)\n",
    "    \n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.xlabel(\"log(satellite)\", fontsize=14)\n",
    "plt.ylabel(\"log($PM_{2.5}$)\", fontsize=14)\n",
    "plt.title(\"WHO super-regions\")\n",
    "plt.xlim(0.5, 7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the different geographic areas only partially overlap, and regression lines fit to each show very different relationships. Gabry *et al* also tried grouping stations using hierarchical clustering (labeled \"cluster_region\" in the dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,5)\n",
    "plt.plot(full_x, full_y, lw=4)\n",
    "\n",
    "for i, m, c in zip(range(1,7), markers, colors):\n",
    "    subset = data[data.cluster_region == i]\n",
    "    plt.scatter(subset.log_sat, subset.log_pm25, marker=m, color=c, alpha=0.5, label=i)\n",
    "    \n",
    "    slope, intercept, *_ = linregress(subset.log_sat, subset.log_pm25)\n",
    "    x = np.array([subset.log_sat.min(), subset.log_sat.max()])\n",
    "    plt.plot(x, slope*x+intercept, c, lw=2)\n",
    "    \n",
    "plt.legend(loc=\"center right\")\n",
    "plt.title(\"Regions grouped with hierarchical clustering\", fontsize=14)\n",
    "plt.xlabel(\"log(satellite)\", fontsize=14)\n",
    "plt.ylabel(\"log($PM_{2.5}$)\", fontsize=14)\n",
    "plt.xlim(0.5, 7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the super-region plot, we see very different dependences here. When we start modeling, we'll need to be on the lookout for biases in predictions on different types of locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "To have something to benchmark against, let's start by fitting a Bayesian linear regression,\n",
    "\n",
    "$y_{i} \\sim N(\\beta_{0} + \\beta_{1}x_{i}, \\sigma^{2})$\n",
    "\n",
    "where $y_{i}$ is the ground station $PM_{2.5}$ measurement, $x_{i}$ is this satellite measurement, and $\\beta_{0}$, $\\beta_{1}$, and $\\sigma$ are the parameters we need to infer. We'll use a normal distribution for priors on $\\beta_{0}$ and $\\beta_{1}$, and a half-normal for $\\sigma$ (since it has to be positive).\n",
    "\n",
    "Before fitting anything, we should do a \"keep yourself honest\" check to see how well we've specified our model. Even without touching stan, it's easy to draw fake data from the prior predictive distribution:\n",
    "\n",
    "1. draw samples $\\beta_{0}^{'}$, $\\beta_{1}^{'}$, and $\\sigma^{'}$ for each of the parameters from their prior distribution\n",
    "* for each data point $x$, compute an estimate $y^{'} = \\beta_{0}^{'} + \\beta_{1}^{'}x + \\epsilon\\sigma^{'})$, where $\\epsilon \\sim N(0,1)$\n",
    "\n",
    "Let's plot some of our prior predictive datasets alongside our actual data, starting with \"weakly informative\" priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12, 8)\n",
    "N = len(data)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    sigma = np.abs(np.random.normal(0,100))\n",
    "    \n",
    "    beta0 = np.random.normal(0, 100)\n",
    "    beta1 = np.random.normal(0, 100)\n",
    "    eps = np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    y_sim = beta0 + beta1*data.log_sat.values + eps\n",
    "\n",
    "    plt.plot(data.log_sat, y_sim, \".\", alpha=0.1)\n",
    "    plt.plot(data.log_sat, data.log_pm25, \".\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these simulations include predictions orders of magnitude away from what we actually observe (remember we're working on log scales here!)! In some cases the slope also goes the wrong way- these priors generate data that's decidedly unphysical. This is bad for two reasons:\n",
    "\n",
    "* It means the prior doesn't *really* represent our prior knowledge, which defeats the entire conceptual point of this branch of statistics\n",
    "* More practically, it means that the data has to do more \"work\" than it should to constrain the prior. We'll tend to get posteriors that overestimate uncertainty and (if we do a *really* bad job specifying the prior) may be biased.\n",
    "\n",
    "So what should we be looking for? From Gabry *et al*,\n",
    "\n",
    "> What do we need in our priors? This suggests we need *containment*: priors that keep us inside sensible parts of the parameter space.\n",
    "\n",
    "So our priors should be broader than (and include) our data- but not be so broad that they make impossible predictions. Let's try again with our intercept constrained to be near zero, slope biased toward being a small poisitive number (since we already know the measurements are roughly correlated) and noise confined to near the unit scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12, 8)\n",
    "N = len(data)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    sigma = np.abs(np.random.normal(0,1))\n",
    "\n",
    "    beta0 = np.random.normal(0, 1)\n",
    "    beta1 = np.random.normal(1, 1)\n",
    "    eps = np.random.normal(0, sigma, size=N)\n",
    "    y_sim = beta0 + beta1*data.log_sat.values + eps\n",
    "\n",
    "    plt.plot(data.log_sat, y_sim, \".\", alpha=0.25)\n",
    "    plt.plot(data.log_sat, data.log_pm25, \".\", alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, some actual inference\n",
    "\n",
    "Here's Gabry *et al's* code for the basic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = \"\"\"\n",
    "/* Simple linear regression */\n",
    "data {\n",
    "  int<lower=1> N;       // number of observations\n",
    "  vector[N] log_sat;    // log of satellite measurements\n",
    "  vector[N] log_pm;     // log of ground PM_2.5 measurements\n",
    "}\n",
    "parameters {\n",
    "  real beta0;           // global intercept\n",
    "  real beta1;           // global slope\n",
    "  real<lower=0> sigma;  // error sd for Gaussian likelihood\n",
    "}\n",
    "model {\n",
    "  // Log-likelihood\n",
    "  target += normal_lpdf(log_pm | beta0 + beta1 * log_sat, sigma);\n",
    "\n",
    "  // Log-priors\n",
    "  target += normal_lpdf(sigma | 0, 1)\n",
    "          + normal_lpdf(beta0 | 0, 1)\n",
    "          + normal_lpdf(beta1 | 1, 1);\n",
    "}\n",
    "generated quantities {\n",
    "  vector[N] log_lik;    // pointwise log-likelihood for LOO\n",
    "  vector[N] log_pm_rep; // replications from posterior predictive dist\n",
    "\n",
    "  for (n in 1:N) {\n",
    "    real log_pm_hat_n = beta0 + beta1 * log_sat[n];\n",
    "    log_lik[n] = normal_lpdf(log_pm[n] | log_pm_hat_n, sigma);\n",
    "    log_pm_rep[n] = normal_rng(log_pm_hat_n, sigma);\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = pystan.StanModel(model_code=model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = {\"N\":len(data), \"log_sat\":data.log_sat.values, \"log_pm\":data.log_pm25.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.sampling(data=datadict, iter=10000, chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract traces of our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0_samples = fit.extract()[\"beta0\"]\n",
    "beta1_samples = fit.extract()[\"beta1\"]\n",
    "sigma_samples = fit.extract()[\"sigma\"]\n",
    "log_pm_rep = fit.extract()[\"log_pm_rep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pm_rep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace plots\n",
    "\n",
    "Gabry's paper has some slick visualizations for investigating your MCMC run, that we'll talk about in the future when we discuss Hamiltonian Monte Carlo. For now, just do some quick trace plots to ensure nothing terrible happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(9,5)\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(sigma_samples)\n",
    "plt.ylabel(\"$\\\\sigma$\", fontsize=14)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(beta0_samples)\n",
    "plt.ylabel(\"$\\\\beta_{0}$\", fontsize=14)\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(beta1_samples)\n",
    "plt.ylabel(\"$\\\\beta_{1}$\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we're mixing OK.\n",
    "\n",
    "Not surprisingly, the slope and intercept show some negative correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(14,4)\n",
    "plt.subplot(131)\n",
    "plt.hexbin(beta0_samples, beta1_samples)\n",
    "plt.ylabel(\"$\\\\beta_{1}$\", fontsize=14)\n",
    "plt.xlabel(\"$\\\\beta_{0}$\", fontsize=14)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hexbin(beta0_samples, sigma_samples)\n",
    "plt.ylabel(\"$\\\\sigma$\", fontsize=14)\n",
    "plt.xlabel(\"$\\\\beta_{0}$\", fontsize=14)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hexbin(beta1_samples, sigma_samples)\n",
    "plt.ylabel(\"$\\\\sigma$\", fontsize=14)\n",
    "plt.xlabel(\"$\\\\beta_{1}$\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Checks\n",
    "\n",
    "Now that we've fit a model, we can draw data from the posterior predictive distribution $p(y^{'}|y)$. We can do a couple types of things with this fake data to get some intuition for how our model is doing:\n",
    "\n",
    "* Directly compare samples from the PPD with real data\n",
    "* Compute summary statistics on the PPD; compare range of values to the same statistic on the observed dataset.\n",
    "\n",
    "In either case we can break those checks out by subsets of the data (like WHO super region) to see if the model describes data equally well in different regions, or whether we'll tend to see geographic biases in any estimates we make from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct comparisons\n",
    "\n",
    "Here's a flipbook of scatter plots, comparing PPD datasets with the real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,8)\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    index = np.random.randint(log_pm_rep.shape[0])\n",
    "    plt.plot(data.log_sat, log_pm_rep[index,:], \"^\", alpha=0.1)\n",
    "    plt.plot(data.log_sat, data.log_pm25, \".\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While they certainly overlap, (as expected) there's some interesting structure in the real data that the PPD doesn't capture. Breaking out by super region exacerbates the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,8)\n",
    "j = 1\n",
    "for i in data.super_region_name.unique():\n",
    "    plt.subplot(3,3,j)\n",
    "    subset = data[data.super_region_name == i]\n",
    "    ppc_subset = log_pm_rep[:,data.super_region_name.values == i]\n",
    "    \n",
    "    plt.plot(subset.log_sat, log_pm_rep[0,data.super_region_name.values == i], \"^\", alpha=0.5)\n",
    "    plt.plot(subset.log_sat, subset.log_pm25, \".\", alpha=0.5)\n",
    "    plt.title(i)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the histogram of observed $PM_{2.5}$ values with samples from the PPD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(7,5)\n",
    "for i in range(500):\n",
    "    plt.hist(log_pm_rep[i,:], histtype=\"step\", color=\"b\", alpha=0.05, bins=25)\n",
    "    \n",
    "plt.hist(data.log_pm25.values, histtype=\"step\", lw=2, color=\"g\", bins=25)\n",
    "plt.ylabel(\"frequency\", fontsize=14)\n",
    "plt.xlabel(\"log($PM_{2.5}$)\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the real data has some asymmetry that the synthetic data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with summary statistics\n",
    "\n",
    "A danger with posterior predictive checking is that we're \"double dipping\" with our data- using it once to fit the model and a second time to check it. When we're comparing summary statistics, a poor choice of statistic could obscure a serious problem with the model.\n",
    "\n",
    "Consider, for example, a model where one parameter controls the mean of the posterior distribution. Comparing means of samples from the PPD will probably look good (because that parameter can learn the mean of your data easily) even if the shape of the fit is terrible. It's good practice, then, to select test statistics that aren't directly related to a single model parameter.\n",
    "\n",
    "In this case- since our histogram showed that the real data is much less symmetric than the posterior predictive distribution, let's use the skewness (a measure of distribution asymmetry):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_skew = skew(data.log_pm25.values)\n",
    "ppc_skews = np.array([skew(log_pm_rep[i,:]) for i in range(log_pm_rep.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(7,5)\n",
    "plt.hist(ppc_skews, label=\"posterior predictive\")\n",
    "ylim = plt.ylim()\n",
    "plt.vlines(observed_skew, -1e6, 1e6, lw=5, label=\"observed\")\n",
    "plt.ylim(ylim)\n",
    "plt.xlabel(\"skewness\", fontsize=14)\n",
    "plt.ylabel(\"frequency\", fontsize=14)\n",
    "plt.legend(loc=\"upper center\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking out a statistic (let's try median this time) by WHO super region, we'll see that not only do the samples not resemble the observed data, but they're wrong in different directions for different regions! If we were answering questions that compared estimates for different geographical regions, this would be a first step to getting really dumb answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,10)\n",
    "j = 1\n",
    "for i in data.super_region_name.unique():\n",
    "    plt.subplot(3,3,j)\n",
    "    subset = data[data.super_region_name == i]\n",
    "    ppc_subset = log_pm_rep[:,data.super_region_name.values == i]\n",
    "    submedian = np.array([np.median(ppc_subset[k,:]) for k in range(ppc_subset.shape[0])])\n",
    "    \n",
    "    plt.hist(submedian, label=\"posterior predictive\")\n",
    "    ylim = plt.ylim()\n",
    "    plt.vlines(np.median(subset.log_pm25.values), -1e6, 1e6, lw=5, label=\"observed\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(i, fontsize=14)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The undiscovered country\n",
    "\n",
    "**What's next?** By this point we should all agree that this problem deserves a model more nuanced than simple linear regression. Build a better model and run some of the same tests to see how it performs!\n",
    "\n",
    "* One option: run separate regressions for different regions or clusters\n",
    "* Another option, in between \"one big stupid regression\" and \"a ton of independent regressions\": build a hierarchical model that has separate parameters for each super region (or cluster), but treats those parameters as draws from a common distribution to be learned. This allows the different regions to use some information from each other while still preserving conditional independence. We'll do a *lot* more with hierarchical models later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also: hierarchical model, ripped off from Gabry's Github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmodel_code = \"\"\"\n",
    "/* Non-centered parameterization (NCP) */\n",
    "\n",
    "functions {\n",
    "  /* Compute N-vector of linear predictors\n",
    "   * \n",
    "   * @param intercept Scalar global intercept.\n",
    "   * @param slope Scalar global slope.\n",
    "   * @param intercept_shifts Vector of group-specific intercept shifts.\n",
    "   * @param intercept_shifts Vector of group-specific slope shifts.\n",
    "   * @param group_ids Integer array of group indexes.\n",
    "   * @param x Vector of predictor values.\n",
    "   * \n",
    "   * @return N-vector of linear predictors (N = num_elements(x)).\n",
    "   * \n",
    "   * @throw Reject if intercept_shifts and slope_shifts don't have length\n",
    "   *        equal to max(group_ids);\n",
    "   * @throw Reject if x doesn't have same number of elements as group_ids. \n",
    "   */\n",
    "  vector yhat(real intercept, \n",
    "              real slope, \n",
    "              vector intercept_shifts, \n",
    "              vector slope_shifts, \n",
    "              int[] group_ids, \n",
    "              vector x) {\n",
    "    if (num_elements(x) != num_elements(group_ids))\n",
    "      reject(\"'x' and 'group_ids' must have same length.\");\n",
    "    if (num_elements(intercept_shifts) != max(group_ids))\n",
    "      reject(\"'intercept_shifts' should have max(group_ids) elements.\");\n",
    "    if (num_elements(slope_shifts) != max(group_ids))\n",
    "      reject(\"'slope_shifts' should have max(group_ids) elements.\");\n",
    "      \n",
    "    return intercept + intercept_shifts[group_ids] \n",
    "            + (slope + slope_shifts[group_ids]) .* x;\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  int<lower=1> N;                   // number of observations\n",
    "  int<lower=1> R;                   // number of super regions\n",
    "  int<lower=1,upper=R> region[N];   // region IDs\n",
    "  vector[N] log_sat;                // log of satellite measurements\n",
    "  vector[N] log_pm;                 // log of ground PM_2.5 measurements\n",
    "}\n",
    "parameters {\n",
    "  real beta0;                       // global intercept         \n",
    "  real beta1;                       // global slope \n",
    "  vector[R] beta0_region_raw;       // 'raw' region intercept offsets for NCP\n",
    "  vector[R] beta1_region_raw;       // 'raw' region slope offsets for NCP\n",
    "  real<lower=0> tau0;               // sd of beta0_region\n",
    "  real<lower=0> tau1;               // sd of beta1_region\n",
    "  real<lower=0> sigma;              // error sd for Gaussian likelihood\n",
    "}\n",
    "transformed parameters {\n",
    "  // Scale and shift raw parameters for NCP \n",
    "  // (but no shift in this case since mean 0)\n",
    "  vector[R] beta0_region = tau0 * beta0_region_raw; \n",
    "  vector[R] beta1_region = tau1 * beta1_region_raw;\n",
    "}\n",
    "model {\n",
    "  // Log-likelihood\n",
    "  vector[N] log_pm_hat = yhat(beta0, beta1, \n",
    "                              beta0_region, beta1_region, \n",
    "                              region, log_sat);\n",
    "  target += normal_lpdf(log_pm | log_pm_hat, sigma);\n",
    "    \n",
    "  // Log-priors\n",
    "  target += normal_lpdf(sigma | 0, 1)\n",
    "          + normal_lpdf(tau0  | 0, 1)\n",
    "          + normal_lpdf(tau1  | 0, 1)\n",
    "          + normal_lpdf(beta0 | 0, 1)\n",
    "          + normal_lpdf(beta1 | 1, 1)\n",
    "          + normal_lpdf(beta0_region_raw | 0, 1)\n",
    "          + normal_lpdf(beta1_region_raw | 0, 1);\n",
    "}\n",
    "generated quantities {\n",
    "  vector[N] log_lik;     // pointwise log-likelihood for LOO\n",
    "  vector[N] log_pm_rep;  // replications from posterior predictive dist\n",
    "\n",
    "  { // local block to not save log_pm_hat\n",
    "    vector[N] log_pm_hat = yhat(beta0, beta1, \n",
    "                                beta0_region, beta1_region, \n",
    "                                region, log_sat);\n",
    "    for (n in 1:N) {\n",
    "      log_lik[n] = normal_lpdf(log_pm[n] | log_pm_hat[n], sigma);\n",
    "      log_pm_rep[n] = normal_rng(log_pm_hat[n], sigma);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
