{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing two ratios\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "This example uses the **binomial distribution** as well as the **beta distribution.** Consider skimming the Wikipedia pages if you're not familiar with them. The beta is conjugate to the binomial so you'll see the two together frequently.\n",
    "\n",
    "## The problem\n",
    "\n",
    "**Note:** This example is ripped off from the excellent repository here: https://jrnold.github.io/bugs-examples-in-stan/undervote\n",
    "\n",
    "The type of question we're going to answer is a common one- if we estimate fraction from different datasets, how do we decide whether a difference is significant? Or whether different datasets are consistent with each other?\n",
    "\n",
    "The specific question is whether there's a racial gap in undervoting in the US presidential race. The data below are counts from two polls (Voter News Service (VNS) exit poll for the 1992 election and American National Election Studies (ANES) for the 1964â€“2000 elections), giving the count (broken out by race) of people who reported voting in the election but didn't fill in a candidate for president.\n",
    "\n",
    "## Bayesian statistics is not the only way to answer this\n",
    "\n",
    "It would be perfectly acceptable to compute standard polling errors and see whether they overlap. There may also be any number of frequentist hypothesis test statistics you could compute for this problem. All are perfectly acceptable approaches.\n",
    "\n",
    "One thing that's convenient about the Bayesian approach is that we can model the thing we're interested (the **difference** in ratios by race) directly. This ability will really shine when we want to build more complicated models (where it'd be difficult to design a frequentist estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Import libraries and set up the data as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pystan\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"survey\":[\"VNS\", \"VNS\", \"ANES\", \"ANES\"],\n",
    "    \"race\":[\"black\", \"white\", \"black\", \"white\"],\n",
    "    \"n\":[6537, 44531, 1101, 9827],\n",
    "    \"undervote\":[26, 91, 10, 57]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an initial point estimate\n",
    "\n",
    "If we didn't know that statistics was a thing, we'd just compute the rates by dividing `undervote` by `n`. Let's do that (and compute differences between race) for each survey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VNS point estimate of delta\n",
    "r_black = 26/6537 \n",
    "r_white = 91/44531\n",
    "print(\"VNS black undervote rate:\", round(r_black,5))\n",
    "print(\"VNS white undervote rate:\", round(r_white,5))\n",
    "\n",
    "delta_vns = r_black - r_white\n",
    "print(\"VNS delta:\", round(delta_vns,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANES point estimate of delta\n",
    "r_black = 10/1101 \n",
    "r_white = 57/9827\n",
    "print(\"ANES black undervote rate:\", round(r_black,5))\n",
    "print(\"ANES white undervote rate:\", round(r_white,5))\n",
    "\n",
    "delta_anes = r_black - r_white\n",
    "print(\"anes delta:\", round(delta_anes,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But unfortunately we do know about statistics. So now that we have a simple answer that was easy to compute, we have to dive into an existential void of self-doubt:\n",
    "\n",
    "* Those deltas are pretty small- 0.2% and 0.3%. Could it be that there's actually no difference and we're looking at sampling error?\n",
    "*  Even if the difference is real- the surveys gave different results? How big of a difference could there be before we'd decide that they're inconsistent with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Let's start formalizing the problem into a model.\n",
    "\n",
    "For survey $i$ and race $j$, there's some (unknown) probability $\\theta_{ij}$ of undervoting. \n",
    "\n",
    "The latent variable we're interested in, for survery $i$, is\n",
    "\n",
    "$\\delta_{i} = \\theta_{i,black} - \\theta_{i,white}$\n",
    "\n",
    "To compute this, we'll need to put a prior over the probabilities $p(\\theta_{ij})$ and relate them to the observed undervote counts $y_{ij}$.\n",
    "\n",
    "Since we're dealing with binary counts, the binomial is a natural distribution to use:\n",
    "\n",
    "$y_{ij} \\sim Bin\\left(N_{ij}, \\theta_{ij}\\right)$\n",
    "\n",
    "For the prior- the Beta distribution is conjugate to the Binomial:\n",
    "\n",
    "$\\theta_{ij} \\sim Beta(a,b)$\n",
    "\n",
    "where $a$ and $b$ are the prior hyperparameters we'll set. Using $(1,1)$ would give us a uniform prior if we want to start simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for a,b in [(1,1), (2,1), (2,2), (5,20)]:\n",
    "    plt.subplot(2,2,i)\n",
    "    _ = plt.hist(np.random.beta(a,b, 10000), bins=np.linspace(0,1,25))\n",
    "    plt.title(\"Beta(%s,%s)\"%(a,b), fontsize=14)\n",
    "    plt.xticks([0,1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a stan model to do inference over the $\\theta$'s. Here's some skeleton code to get you started:\n",
    "\n",
    "```\n",
    "model_code = \"\"\"\n",
    "\n",
    "data {\n",
    "  // you'll need variables for n and y.\n",
    "  // both will need a [4] after them.\n",
    "}\n",
    "parameters {\n",
    "  // the thetas are the only parameters. same deal on the [4].\n",
    "  // also, I had initialization errors until I manually constrained\n",
    "  // the lower and upper bounds.\n",
    "}\n",
    "model {\n",
    "  // write the prior over theta, and\n",
    "  // the likelihood of y as a function of n and theta\n",
    "}\n",
    "generated quantities {\n",
    "  // finally, just for convenience- define delta_vns and \n",
    "  // delta_anes in terms of the thetas.\n",
    "  real delta_vns = theta[1] - theta[2];\n",
    "  real delta_anes = theta[3] - theta[4];\n",
    "}\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = \"\"\"\n",
    "DO YOUR THING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = pystan.StanModel(model_code=model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"n\":df.n.values, \"y\":df.undervote.values}\n",
    "fit = model.sampling(data=data, iter=5000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace diagnostics\n",
    "\n",
    "We don't expect any computational issues with such a simple model, but since it's a good habit to be in we'll check anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = fit.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces[\"theta\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.plot(traces[\"theta\"][:,i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the results\n",
    "\n",
    "The values in the trace, are samples from $Pr(\\delta_{vns}|X)$ and $Pr(\\delta_{anes}|X)$, where $X$ is the set of observations.\n",
    "\n",
    "We can do a quick visual check to see whether the two posteriors are consistent with each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(traces[\"delta_vns\"], bins=100, alpha=0.5, label=\"$\\delta_{VNS}$\")\n",
    "plt.hist(traces[\"delta_anes\"], bins=100, alpha=0.5, label=\"$\\delta_{ANES}$\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These surveys were conducted by different companies over different people in different times- so there could be a lot of reasons why they may not line up, even if the underlying effect was real and consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have samples from the posterior, it's straightforward to make probabilistic assessments about different hypotheses- we can frame different questions depending on the decision we want to make from our analysis. For example:\n",
    "\n",
    "What's the probability that the delta is actually zero or negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"delta_vns:\", np.mean(traces[\"delta_vns\"] <= 0))\n",
    "print(\"delta_anes:\", np.mean(traces[\"delta_anes\"] <= 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the probability that the size of the delta (in either direction) is less than a tenth of a percent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"delta_vns:\", np.mean(np.abs(traces[\"delta_vns\"]) < 0.001))\n",
    "print(\"delta_anes:\", np.mean(np.abs(traces[\"delta_anes\"]) < 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
