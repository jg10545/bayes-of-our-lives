{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier-resistant regression\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "This example uses the **normal**, **student's t**, **cauchy**, and **gamma** distributions. A couple notes:\n",
    "\n",
    "* if you're going to look up just one of these, look up the normal. seriously you need this one.\n",
    "* student's t shows up more often in frequentist statistics- we'll use it as a variant on the normal that we can tune to have heavier tails\n",
    "* cauchy is a heavy-tailed distribution; we'll use it here for a broad prior. If you're interested in modeling critical phenomena this is a good tool to have in your belt.\n",
    "* gamma is another one we'll just use here for a prior. If you're interested in event modeling in continuous time it's worth reading up on (for a Poisson process, it'll be the distribution of waiting times before N events happen). note that there are two ways to parameterize it (wikipedia has both); stan uses one and numpy uses the other.\n",
    "\n",
    "## The problem\n",
    "\n",
    "**Note:** This example is modified off from the excellent repository here: https://jrnold.github.io/bugs-examples-in-stan/resistant#ref-Jackman2000a\n",
    "\n",
    "One of the great things about Bayesian modeling is that it's usually pretty easy (if you're using MCMC) to take a standard model and customize it for your problem. So we're going to take a dataset for which the standard \"normally-distributed errors\" assumption doesn't quite work, and just swap out a more convenient distribuion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pystan\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "This dataset was collected by Douglas Grob on incumbency advantage in American congressional elections, 1956-1994 (Jackman 2000). The response variable is the percentage of the vote that went to the Democratic candidate; the covariates include the lagged value and whether a Democrat or Republican was the incumbent. There's also a year indicator that we'll allow to fit as a fixed-effect for each year, since we know there are correlations in how people vote each year (like the Democratic wave electing Obama in 2008 and the midterms in the opposite direction in 2010).\n",
    "\n",
    "Jackman, Simon. 2000. *“Estimation and Inference Are Missing Data Problems: Unifying Social Science Statistics via Bayesian Simulation.”* Political Analysis 8 (4). [Oxford University Press, Society for Political Methodology]: 307–32. http://www.jstor.org/stable/25791616."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"resistant.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.year.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a quick look at the distribution of response variables, broken out by categories in the data. In particular- check out that very-not-normal-looking tail in the middle plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,4)\n",
    "cols = [\"prvwinpty\", \"deminc\", \"repinc\"]\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    col = cols[i]\n",
    "    for v in df[col].unique():\n",
    "        plt.hist(df[df[col] == v].y.values, bins=50, alpha=0.5, label=v)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(\"y broken out by %s\"%col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm in a hurry; let's just do normal linear regression like chumps\n",
    "\n",
    "Let's code up a boring linear regression so we have something to compare against.\n",
    "\n",
    "I'm also going to split the data up into train and test sets so we can evaluate on out-of-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat dummy variables for each year, and pull out numpy arrays for covariates and dependent variables\n",
    "X = np.concatenate([df[[\"lagy\", \"prvwinpty\", \"deminc\", \"repinc\"]].values,\n",
    "              pd.get_dummies(df.year).values], axis=1)\n",
    "Y = df.y.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random 70/30 train/test split\n",
    "np.random.seed(1)\n",
    "test = np.random.choice([False, True], p=[0.7,0.3], size=len(df))\n",
    "X_test = X[test]\n",
    "Y_test = Y[test]\n",
    "\n",
    "X = X[~test,:]\n",
    "Y = Y[~test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_model_code = \"\"\"\n",
    "data {\n",
    "  // DATA INPUTS\n",
    "  // number of data points\n",
    "  int N;\n",
    "  // target variable y is a vector; one value for each data point\n",
    "  vector[N] y;\n",
    "  // number of covariates\n",
    "  int K;\n",
    "  // design matrix X contains all our covariate data- one row per data point \n",
    "  // and one column per covariate\n",
    "  matrix[N, K] X;\n",
    "  \n",
    "  // MODEL INPUTS\n",
    "  // prior scale for the error distribution \n",
    "  real sigma_scale;\n",
    "  // prior scale for the regression coefficients\n",
    "  vector[K] beta_scale;\n",
    "}\n",
    "parameters {\n",
    "  // one coefficient for each covariate\n",
    "  vector[K] beta;\n",
    "  // one for the intercept\n",
    "  real intercept;\n",
    "  // and one for the noise distribution\n",
    "  real sigma;\n",
    "}\n",
    "model{\n",
    "  // priors for error variance\n",
    "  sigma ~ cauchy(0., sigma_scale);\n",
    "  \n",
    "  // priors for the regression coefficients\n",
    "  beta ~ normal(0, beta_scale);\n",
    "  intercept ~ normal(0, sigma_scale);\n",
    "  // likelihood\n",
    "  y ~ normal(X*beta + intercept, sigma);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_norm = pystan.StanModel(model_code=normal_model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "K = X.shape[1]\n",
    "data = {\"y\":Y,\n",
    "        \"X\":X,\n",
    "        \"N\":X.shape[0],\n",
    "       \"K\":K,\n",
    "       #\"beta_loc\":np.zeros(K),\n",
    "       \"beta_scale\":2.5*np.std(Y)*np.ones(K),\n",
    "       \"sigma_scale\":5*np.std(Y)}\n",
    "\n",
    "fit_norm = model_norm.sampling(data=data, iter=1000, chains=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_norm = fit_norm.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick posterior predictive check- what we'll see is that the regression model generally captures the shape of the data, but (not surprisingly) is missing some of the more extreme values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,12)\n",
    "for j in range(9):\n",
    "    i = np.random.choice(np.arange(traces_norm[\"beta\"].shape[0]))\n",
    "    beta = traces_norm[\"beta\"][i,:]\n",
    "    sigma = traces_norm[\"sigma\"][i]\n",
    "    intercept = traces_norm[\"intercept\"][i]\n",
    "    epsilon = np.random.normal(0, sigma, size=len(Y_test))\n",
    "    \n",
    "\n",
    "    plt.subplot(3,3,j+1)\n",
    "    plt.plot(X_test[:,0], Y_test, \"o\", alpha=0.5)\n",
    "    plt.plot(X_test[:,0], beta.dot(X_test.T)+intercept+epsilon, \".\", alpha=0.5)\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So depending on what we wanted to do with this regression model, this may or may not be a problem. Let's try fitting a model that can handle those heavy tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now modify to be more robust against outliers\n",
    "\n",
    "We'll use Student's T distribution to replace the usual normal likelihood for regression. The T has an extra parameter \"degrees of freedom\" that lets us tune it- as the DOF approaches infinity, the T distribution becomes the normal; as it gets closer to zero the tails get increasingly fatter. Of course, we don't know the \"true\" value for the DOF, but since we're good Bayesians we can just put a prior distribution over anything we don't know.\n",
    "\n",
    "So here's what you should do:\n",
    "\n",
    "* start with the normal regression code\n",
    "* add a `real` parameter `nu` to hold the degrees of freedom\n",
    "* put a `gamma` prior over `nu` (I used `gamma(2, 0.5)`, but hey, you do you)\n",
    "* replace the `normal` likelihood with a `student_t` likelihood. The `student_t` stan function takes three inputs- degrees of freedom, mean, and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model_code = \"\"\"\n",
    "DO STUFF HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_t = pystan.StanModel(model_code=t_model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "K = X.shape[1]\n",
    "data = {\"y\":Y,\n",
    "        \"X\":X,\n",
    "        \"N\":X.shape[0],\n",
    "       \"K\":K,\n",
    "       #\"beta_loc\":np.zeros(K),\n",
    "       \"beta_scale\":2.5*np.std(Y)*np.ones(K),\n",
    "       \"sigma_scale\":5*np.std(Y)}\n",
    "\n",
    "fit_t = model_t.sampling(data=data, iter=1000, chains=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_t = fit_t.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value of the degrees-of-freedom parameter did the model think was consistent with the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(7,4)\n",
    "plt.hist(np.random.gamma(2, 1./0.5, size=len(traces_t[\"nu\"])), bins=100, label=\"prior\", alpha=0.5)\n",
    "plt.hist(traces_t[\"nu\"], bins=100, label=\"posterior\", alpha=0.5)\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior predictive checks\n",
    "\n",
    "Let's do a posterior predictive check like we did before for the normal model- look around the edges of the point cloud; this model should generate more-realistic samples that occasionally include an outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,12)\n",
    "for j in range(9):\n",
    "    i = np.random.choice(np.arange(traces_t[\"beta\"].shape[0]))\n",
    "    beta = traces_t[\"beta\"][i,:]\n",
    "    intercept = traces_t[\"intercept\"][i]\n",
    "    nu = traces_t[\"nu\"][i]\n",
    "    sigma = traces_t[\"sigma\"][i]\n",
    "    epsilon = sigma*np.random.standard_t(nu, size=len(Y_test))\n",
    "    \n",
    "    plt.subplot(3,3,j+1)\n",
    "    plt.plot(X_test[:,0], Y_test, \"o\", alpha=0.5)\n",
    "    plt.plot(X_test[:,0], beta.dot(X_test.T)+intercept+epsilon, \".\", alpha=0.5)\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another posterior predictive check- let's compare, for both models, the distribution of **residuals** (the difference between the true value and model prediction) on real data and on generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMAL MODEL\n",
    "# pick a sample from the posterior\n",
    "i = np.random.choice(np.arange(traces_norm[\"beta\"].shape[0]))\n",
    "beta = traces_norm[\"beta\"][i,:]\n",
    "sigma = traces_norm[\"sigma\"][i]\n",
    "intercept = traces_norm[\"intercept\"][i]\n",
    "# compute real-data residuals\n",
    "resid_norm = Y_test - beta.dot(X_test.T) - intercept\n",
    "# compute synthetic-data residuals\n",
    "epsilon = np.random.normal(0, sigma, size=len(Y_test))\n",
    "resid_pp_norm = np.random.normal(0, sigma, size=100*len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T MODEL\n",
    "# pick a sample from the posterior\n",
    "i = np.random.choice(np.arange(traces_t[\"beta\"].shape[0]))\n",
    "beta = traces_t[\"beta\"][i,:]\n",
    "intercept = traces_t[\"intercept\"][i]\n",
    "nu = traces_t[\"nu\"][i]\n",
    "sigma = traces_t[\"sigma\"][i]\n",
    "# compute the synthetic-data residual\n",
    "resid_t = Y_test - beta.dot(X_test.T) - intercept\n",
    "# compute synthetic-data residuals\n",
    "resid_pp_t = sigma*np.random.standard_t(nu, size=100*len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the edges of these distributions when we plot a histogram. I found that using a log transformation helped make it more clear.\n",
    "\n",
    "Given that the two models have basically the same predictive accuracy, why would we even care about the differing shapes of error distributions? Imagine that we wanted to use the model for anomaly detection- the normal model would have an annoyingly high false-positive rate, because it would always flag the outliers in the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,5)\n",
    "plt.subplot(121)\n",
    "plt.hist(resid_norm, bins=100, alpha=0.5, density=True, label=\"residual\", log=True)\n",
    "plt.hist(resid_pp_norm, bins=100, alpha=0.5, density=True, label=\"PP residual\", log=True)\n",
    "plt.title(\"Posterior predictive check- normal model\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(resid_t, bins=100, alpha=0.5, density=True, label=\"residual\", log=True)\n",
    "plt.hist(resid_pp_t, bins=100, alpha=0.5, density=True, label=\"PP residual\", log=True)\n",
    "plt.title(\"Posterior predictive check- Student's t model\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the models agree on the coefficients?\n",
    "\n",
    "What if our goal is inference rather than prediction?\n",
    "\n",
    "When I ran this, most of the distributions overlapped- so the models *qualitatively* agreed on what the correlates were; if you were concerned with the *quantitative* relationships, however (\"how many extra points do we get in next year's election per point of spread in the previous election?\"), you'd get different answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12,4)\n",
    "cols = [\"lagy\", \"prvwinpty\", \"deminc\", \"repinc\"]\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.hist(traces_norm[\"beta\"][:,i], alpha=0.5, label=\"normal\")\n",
    "    plt.hist(traces_t[\"beta\"][:,i], alpha=0.5, label=\"student's t\")\n",
    "    plt.title(cols[i])\n",
    "    plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
